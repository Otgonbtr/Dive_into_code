{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQMY125uzOpKPT9A03rq6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otgonbtr/Dive_into_code/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "kOgptNlpYoQU"
      },
      "outputs": [],
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    no_bias : bool\n",
        "      True if the bias term is not included\n",
        "    verbose : bool\n",
        "      学習過程を出力する場合はTrue\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
        "      パラメータ\n",
        "    self.loss : 次の形のndarray, shape (self.iter,)\n",
        "      訓練データに対する損失の記録\n",
        "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
        "      検証データに対する損失の記録\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "        \n",
        "    def fit(self, X, y, X_val, y_val, no_bias=False, verbose=False ):\n",
        "        \"\"\"\n",
        "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        \"\"\"\n",
        "        if self.no_bias == True:\n",
        "\t          bias = np.ones((X.shape[0], 1))\n",
        "\t          X = np.hstack((bias, X))\n",
        "\t          bias = np.ones((X_val.shape[0], 1))\n",
        "\t          X_val = np.hstack((bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.iter):\n",
        "            pred = self.linear_hypothesis(X)\n",
        "            pred_val = self.linear_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if verbose == True:\n",
        "              print('{} learning loss is {}'.format(i,loss))\n",
        "    \n",
        "\n",
        "    def linear_hypothesis(self, X):\n",
        "        \"\"\"\n",
        "        仮定関数の出力を計算する\n",
        "          Parameters\n",
        "          ----------\n",
        "          X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データ\n",
        "          Returns\n",
        "          -------\n",
        "          次の形のndarray, shape (n_samples, 1)\n",
        "          線形の仮定関数による推定結果\n",
        "          \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def _gradient_descent(self,X,y):\n",
        "        \"\"\"\n",
        "        最急降下法によるパラメータの更新値計算\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        n = X.shape[1]\n",
        "        pred = self.linear_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            self.theta[j] = self.theta[j] - self.lr * (gradient / m)\n",
        "        \n",
        "    def predict(self, X,no_bias=False):\n",
        "        \"\"\"\n",
        "        線形回帰を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "            次の形のndarray, shape (n_samples, 1)\n",
        "            線形回帰による推定結果\n",
        "        \"\"\"\n",
        "        if self.no_bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self.linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def _mse(self,y_pred,y,X):\n",
        "        \"\"\"\n",
        "        平均二乗誤差の計算\n",
        "        \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self,y_pred,y):\n",
        "        \"\"\"\n",
        "        損失関数\n",
        "        \"\"\"\n",
        "        loss = self._mse(y_pred, y,X)/2\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "jha-Z8uymyFZ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test, verbose=True)"
      ],
      "metadata": {
        "id": "fZW2KlL3nIbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe4266a-e4cc-46a2-a07d-0498d7f981ef"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 learning loss is 13625103445.821918\n",
            "1 learning loss is 13365050936.320982\n",
            "2 learning loss is 13110299903.423037\n",
            "3 learning loss is 12860741832.436092\n",
            "4 learning loss is 12616270439.880634\n",
            "5 learning loss is 12376781627.382494\n",
            "6 learning loss is 12142173436.523796\n",
            "7 learning loss is 11912346004.631918\n",
            "8 learning loss is 11687201521.48685\n",
            "9 learning loss is 11466644186.927795\n",
            "10 learning loss is 11250580169.3402\n",
            "11 learning loss is 11038917565.004803\n",
            "12 learning loss is 10831566358.290733\n",
            "13 learning loss is 10628438382.675009\n",
            "14 learning loss is 10429447282.571165\n",
            "15 learning loss is 10234508475.950155\n",
            "16 learning loss is 10043539117.73692\n",
            "17 learning loss is 9856458063.966516\n",
            "18 learning loss is 9673185836.68388\n",
            "19 learning loss is 9493644589.57173\n",
            "20 learning loss is 9317758074.291443\n",
            "21 learning loss is 9145451607.521976\n",
            "22 learning loss is 8976652038.682312\n",
            "23 learning loss is 8811287718.323164\n",
            "24 learning loss is 8649288467.17397\n",
            "25 learning loss is 8490585545.83152\n",
            "26 learning loss is 8335111625.07684\n",
            "27 learning loss is 8182800756.807238\n",
            "28 learning loss is 8033588345.57067\n",
            "29 learning loss is 7887411120.689911\n",
            "30 learning loss is 7744207108.964196\n",
            "31 learning loss is 7603915607.936331\n",
            "32 learning loss is 7466477159.713481\n",
            "33 learning loss is 7331833525.330102\n",
            "34 learning loss is 7199927659.641732\n",
            "35 learning loss is 7070703686.738571\n",
            "36 learning loss is 6944106875.868051\n",
            "37 learning loss is 6820083617.855768\n",
            "38 learning loss is 6698581402.014446\n",
            "39 learning loss is 6579548793.530712\n",
            "40 learning loss is 6462935411.319804\n",
            "41 learning loss is 6348691906.338439\n",
            "42 learning loss is 6236769940.346305\n",
            "43 learning loss is 6127122165.106865\n",
            "44 learning loss is 6019702202.018321\n",
            "45 learning loss is 5914464622.165799\n",
            "46 learning loss is 5811364926.785986\n",
            "47 learning loss is 5710359528.135656\n",
            "48 learning loss is 5611405730.755681\n",
            "49 learning loss is 5514461713.122302\n",
            "50 learning loss is 5419486509.677624\n",
            "51 learning loss is 5326439993.231433\n",
            "52 learning loss is 5235282857.726662\n",
            "53 learning loss is 5145976601.360879\n",
            "54 learning loss is 5058483510.056481\n",
            "55 learning loss is 4972766641.272298\n",
            "56 learning loss is 4888789808.149527\n",
            "57 learning loss is 4806517563.98507\n",
            "58 learning loss is 4725915187.025466\n",
            "59 learning loss is 4646948665.574751\n",
            "60 learning loss is 4569584683.409754\n",
            "61 learning loss is 4493790605.4964285\n",
            "62 learning loss is 4419534464.000976\n",
            "63 learning loss is 4346784944.589654\n",
            "64 learning loss is 4275511373.011268\n",
            "65 learning loss is 4205683701.9564886\n",
            "66 learning loss is 4137272498.188251\n",
            "67 learning loss is 4070248929.9376125\n",
            "68 learning loss is 4004584754.5595665\n",
            "69 learning loss is 3940252306.443426\n",
            "70 learning loss is 3877224485.172485\n",
            "71 learning loss is 3815474743.9278073\n",
            "72 learning loss is 3754977078.131074\n",
            "73 learning loss is 3695706014.3215337\n",
            "74 learning loss is 3637636599.26221\n",
            "75 learning loss is 3580744389.2706137\n",
            "76 learning loss is 3525005439.7693067\n",
            "77 learning loss is 3470396295.0517697\n",
            "78 learning loss is 3416893978.259102\n",
            "79 learning loss is 3364475981.563209\n",
            "80 learning loss is 3313120256.552168\n",
            "81 learning loss is 3262805204.813628\n",
            "82 learning loss is 3213509668.7121105\n",
            "83 learning loss is 3165212922.3562074\n",
            "84 learning loss is 3117894662.7517586\n",
            "85 learning loss is 3071535001.1371403\n",
            "86 learning loss is 3026114454.4969077\n",
            "87 learning loss is 2981613937.2501063\n",
            "88 learning loss is 2938014753.109621\n",
            "89 learning loss is 2895298587.1090446\n",
            "90 learning loss is 2853447497.7935915\n",
            "91 learning loss is 2812443909.5716615\n",
            "92 learning loss is 2772270605.22374\n",
            "93 learning loss is 2732910718.5653763\n",
            "94 learning loss is 2694347727.261054\n",
            "95 learning loss is 2656565445.785842\n",
            "96 learning loss is 2619548018.531759\n",
            "97 learning loss is 2583279913.055877\n",
            "98 learning loss is 2547745913.467222\n",
            "99 learning loss is 2512931113.949608\n",
            "100 learning loss is 2478820912.4175997\n",
            "101 learning loss is 2445401004.3028536\n",
            "102 learning loss is 2412657376.468139\n",
            "103 learning loss is 2380576301.246407\n",
            "104 learning loss is 2349144330.602327\n",
            "105 learning loss is 2318348290.413756\n",
            "106 learning loss is 2288175274.870674\n",
            "107 learning loss is 2258612640.989152\n",
            "108 learning loss is 2229648003.2379827\n",
            "109 learning loss is 2201269228.2756453\n",
            "110 learning loss is 2173464429.7953415\n",
            "111 learning loss is 2146221963.4758475\n",
            "112 learning loss is 2119530422.0360267\n",
            "113 learning loss is 2093378630.3908432\n",
            "114 learning loss is 2067755640.9067972\n",
            "115 learning loss is 2042650728.7547233\n",
            "116 learning loss is 2018053387.3579507\n",
            "117 learning loss is 1993953323.9338486\n",
            "118 learning loss is 1970340455.126847\n",
            "119 learning loss is 1947204902.731034\n",
            "120 learning loss is 1924536989.5004869\n",
            "121 learning loss is 1902327235.0455353\n",
            "122 learning loss is 1880566351.8131762\n",
            "123 learning loss is 1859245241.1499143\n",
            "124 learning loss is 1838354989.4453244\n",
            "125 learning loss is 1817886864.3546772\n",
            "126 learning loss is 1797832311.0989947\n",
            "127 learning loss is 1778182948.840951\n",
            "128 learning loss is 1758930567.1350408\n",
            "129 learning loss is 1740067122.4504995\n",
            "130 learning loss is 1721584734.7654757\n",
            "131 learning loss is 1703475684.2309778\n",
            "132 learning loss is 1685732407.9031768\n",
            "133 learning loss is 1668347496.5426426\n",
            "134 learning loss is 1651313691.479141\n",
            "135 learning loss is 1634623881.5406492\n",
            "136 learning loss is 1618271100.0452576\n",
            "137 learning loss is 1602248521.8546734\n",
            "138 learning loss is 1586549460.488053\n",
            "139 learning loss is 1571167365.2949245\n",
            "140 learning loss is 1556095818.6859822\n",
            "141 learning loss is 1541328533.4205723\n",
            "142 learning loss is 1526859349.9496896\n",
            "143 learning loss is 1512682233.8133569\n",
            "144 learning loss is 1498791273.0912604\n",
            "145 learning loss is 1485180675.9055564\n",
            "146 learning loss is 1471844767.9747581\n",
            "147 learning loss is 1458777990.2176802\n",
            "148 learning loss is 1445974896.4063861\n",
            "149 learning loss is 1433430150.8671458\n",
            "150 learning loss is 1421138526.2284184\n",
            "151 learning loss is 1409094901.2148829\n",
            "152 learning loss is 1397294258.4865847\n",
            "153 learning loss is 1385731682.522258\n",
            "154 learning loss is 1374402357.5459285\n",
            "155 learning loss is 1363301565.4959009\n",
            "156 learning loss is 1352424684.0352595\n",
            "157 learning loss is 1341767184.6030388\n",
            "158 learning loss is 1331324630.5052176\n",
            "159 learning loss is 1321092675.0447273\n",
            "160 learning loss is 1311067059.6896715\n",
            "161 learning loss is 1301243612.2789707\n",
            "162 learning loss is 1291618245.2646677\n",
            "163 learning loss is 1282186953.9901369\n",
            "164 learning loss is 1272945815.003466\n",
            "165 learning loss is 1263890984.4052856\n",
            "166 learning loss is 1255018696.2303417\n",
            "167 learning loss is 1246325260.862113\n",
            "168 learning loss is 1237807063.4798095\n",
            "169 learning loss is 1229460562.5370703\n",
            "170 learning loss is 1221282288.271725\n",
            "171 learning loss is 1213268841.2459733\n",
            "172 learning loss is 1205416890.9163673\n",
            "173 learning loss is 1197723174.232975\n",
            "174 learning loss is 1190184494.2671354\n",
            "175 learning loss is 1182797718.8672175\n",
            "176 learning loss is 1175559779.3418071\n",
            "177 learning loss is 1168467669.169762\n",
            "178 learning loss is 1161518442.7365804\n",
            "179 learning loss is 1154709214.0965512\n",
            "180 learning loss is 1148037155.7601526\n",
            "181 learning loss is 1141499497.506181\n",
            "182 learning loss is 1135093525.2181096\n",
            "183 learning loss is 1128816579.7441728\n",
            "184 learning loss is 1122666055.780697\n",
            "185 learning loss is 1116639400.7782001\n",
            "186 learning loss is 1110734113.8697906\n",
            "187 learning loss is 1104947744.8214123\n",
            "188 learning loss is 1099277893.0034912\n",
            "189 learning loss is 1093722206.3835366\n",
            "190 learning loss is 1088278380.5392804\n",
            "191 learning loss is 1082944157.6919217\n",
            "192 learning loss is 1077717325.7590737\n",
            "193 learning loss is 1072595717.4270103\n",
            "194 learning loss is 1067577209.2418079\n",
            "195 learning loss is 1062659720.7190108\n",
            "196 learning loss is 1057841213.4714261\n",
            "197 learning loss is 1053119690.3546891\n",
            "198 learning loss is 1048493194.6302307\n",
            "199 learning loss is 1043959809.1452878\n",
            "200 learning loss is 1039517655.529617\n",
            "201 learning loss is 1035164893.4085634\n",
            "202 learning loss is 1030899719.6321473\n",
            "203 learning loss is 1026720367.519855\n",
            "204 learning loss is 1022625106.1207952\n",
            "205 learning loss is 1018612239.4889207\n",
            "206 learning loss is 1014680105.9729984\n",
            "207 learning loss is 1010827077.5210352\n",
            "208 learning loss is 1007051558.9988552\n",
            "209 learning loss is 1003351987.5225443\n",
            "210 learning loss is 999726831.8044798\n",
            "211 learning loss is 996174591.5126638\n",
            "212 learning loss is 992693796.6430918\n",
            "213 learning loss is 989283006.9048882\n",
            "214 learning loss is 985940811.11795\n",
            "215 learning loss is 982665826.6228415\n",
            "216 learning loss is 979456698.7026914\n",
            "217 learning loss is 976312100.0168447\n",
            "218 learning loss is 973230730.0460346\n",
            "219 learning loss is 970211314.5488319\n",
            "220 learning loss is 967252605.0291504\n",
            "221 learning loss is 964353378.2145729\n",
            "222 learning loss is 961512435.5452868\n",
            "223 learning loss is 958728602.6734052\n",
            "224 learning loss is 956000728.9724673\n",
            "225 learning loss is 953327687.0569035\n",
            "226 learning loss is 950708372.3112721\n",
            "227 learning loss is 948141702.4290571\n",
            "228 learning loss is 945626616.9608403\n",
            "229 learning loss is 943162076.8716525\n",
            "230 learning loss is 940747064.1073189\n",
            "231 learning loss is 938380581.1696126\n",
            "232 learning loss is 936061650.7000405\n",
            "233 learning loss is 933789315.0720793\n",
            "234 learning loss is 931562635.9916989\n",
            "235 learning loss is 929380694.1059936\n",
            "236 learning loss is 927242588.6197647\n",
            "237 learning loss is 925147436.9198865\n",
            "238 learning loss is 923094374.2072996\n",
            "239 learning loss is 921082553.136478\n",
            "240 learning loss is 919111143.4622127\n",
            "241 learning loss is 917179331.6935674\n",
            "242 learning loss is 915286320.7548574\n",
            "243 learning loss is 913431329.6535103\n",
            "244 learning loss is 911613593.1546676\n",
            "245 learning loss is 909832361.4623907\n",
            "246 learning loss is 908086899.9073336\n",
            "247 learning loss is 906376488.6407562\n",
            "248 learning loss is 904700422.3347449\n",
            "249 learning loss is 903058009.8885136\n",
            "250 learning loss is 901448574.1406654\n",
            "251 learning loss is 899871451.5872905\n",
            "252 learning loss is 898325992.1057813\n",
            "253 learning loss is 896811558.6842505\n",
            "254 learning loss is 895327527.1564356\n",
            "255 learning loss is 893873285.9419812\n",
            "256 learning loss is 892448235.7919872\n",
            "257 learning loss is 891051789.5397149\n",
            "258 learning loss is 889683371.8563508\n",
            "259 learning loss is 888342419.0117204\n",
            "260 learning loss is 887028378.6398522\n",
            "261 learning loss is 885740709.5092949\n",
            "262 learning loss is 884478881.2980922\n",
            "263 learning loss is 883242374.3733146\n",
            "264 learning loss is 882030679.5750632\n",
            "265 learning loss is 880843298.004848\n",
            "266 learning loss is 879679740.8182567\n",
            "267 learning loss is 878539529.0218248\n",
            "268 learning loss is 877422193.274018\n",
            "269 learning loss is 876327273.6902525\n",
            "270 learning loss is 875254319.6518601\n",
            "271 learning loss is 874202889.6189265\n",
            "272 learning loss is 873172550.9469192\n",
            "273 learning loss is 872162879.7070309\n",
            "274 learning loss is 871173460.5101601\n",
            "275 learning loss is 870203886.3344582\n",
            "276 learning loss is 869253758.3563673\n",
            "277 learning loss is 868322685.7850814\n",
            "278 learning loss is 867410285.700356\n",
            "279 learning loss is 866516182.8936069\n",
            "280 learning loss is 865640009.712221\n",
            "281 learning loss is 864781405.9070215\n",
            "282 learning loss is 863940018.4828213\n",
            "283 learning loss is 863115501.5519993\n",
            "284 learning loss is 862307516.1910424\n",
            "285 learning loss is 861515730.2999903\n",
            "286 learning loss is 860739818.4647247\n",
            "287 learning loss is 859979461.8220452\n",
            "288 learning loss is 859234347.9274762\n",
            "289 learning loss is 858504170.6257478\n",
            "290 learning loss is 857788629.9238975\n",
            "291 learning loss is 857087431.8669387\n",
            "292 learning loss is 856400288.4160464\n",
            "293 learning loss is 855726917.3292042\n",
            "294 learning loss is 855067042.0442688\n",
            "295 learning loss is 854420391.5643955\n",
            "296 learning loss is 853786700.3457857\n",
            "297 learning loss is 853165708.187701\n",
            "298 learning loss is 852557160.1247036\n",
            "299 learning loss is 851960806.3210747\n",
            "300 learning loss is 851376401.9673673\n",
            "301 learning loss is 850803707.1790527\n",
            "302 learning loss is 850242486.8972142\n",
            "303 learning loss is 849692510.7912487\n",
            "304 learning loss is 849153553.1635357\n",
            "305 learning loss is 848625392.8560339\n",
            "306 learning loss is 848107813.1587644\n",
            "307 learning loss is 847600601.7201438\n",
            "308 learning loss is 847103550.4591304\n",
            "309 learning loss is 846616455.4791459\n",
            "310 learning loss is 846139116.9837347\n",
            "311 learning loss is 845671339.1939306\n",
            "312 learning loss is 845212930.2672895\n",
            "313 learning loss is 844763702.2185612\n",
            "314 learning loss is 844323470.8419617\n",
            "315 learning loss is 843892055.6350175\n",
            "316 learning loss is 843469279.7239459\n",
            "317 learning loss is 843054969.7905453\n",
            "318 learning loss is 842648956.0005615\n",
            "319 learning loss is 842251071.9334984\n",
            "320 learning loss is 841861154.5138495\n",
            "321 learning loss is 841479043.9437165\n",
            "322 learning loss is 841104583.6367888\n",
            "323 learning loss is 840737620.1536567\n",
            "324 learning loss is 840378003.138429\n",
            "325 learning loss is 840025585.2566333\n",
            "326 learning loss is 839680222.1343665\n",
            "327 learning loss is 839341772.2986768\n",
            "328 learning loss is 839010097.1191473\n",
            "329 learning loss is 838685060.7506601\n",
            "330 learning loss is 838366530.077313\n",
            "331 learning loss is 838054374.6574732\n",
            "332 learning loss is 837748466.669934\n",
            "333 learning loss is 837448680.8611624\n",
            "334 learning loss is 837154894.4936099\n",
            "335 learning loss is 836866987.2950652\n",
            "336 learning loss is 836584841.4090315\n",
            "337 learning loss is 836308341.3461018\n",
            "338 learning loss is 836037373.9363174\n",
            "339 learning loss is 835771828.2824878\n",
            "340 learning loss is 835511595.7144505\n",
            "341 learning loss is 835256569.7442547\n",
            "342 learning loss is 835006646.0222487\n",
            "343 learning loss is 834761722.2940537\n",
            "344 learning loss is 834521698.3584034\n",
            "345 learning loss is 834286476.025837\n",
            "346 learning loss is 834055959.0782217\n",
            "347 learning loss is 833830053.2290952\n",
            "348 learning loss is 833608666.0848035\n",
            "349 learning loss is 833391707.1064267\n",
            "350 learning loss is 833179087.57247\n",
            "351 learning loss is 832970720.5423069\n",
            "352 learning loss is 832766520.82036\n",
            "353 learning loss is 832566404.9210035\n",
            "354 learning loss is 832370291.0341742\n",
            "355 learning loss is 832178098.9916748\n",
            "356 learning loss is 831989750.2341563\n",
            "357 learning loss is 831805167.7787653\n",
            "358 learning loss is 831624276.1874452\n",
            "359 learning loss is 831447001.5358751\n",
            "360 learning loss is 831273271.3830342\n",
            "361 learning loss is 831103014.7413805\n",
            "362 learning loss is 830936162.0476289\n",
            "363 learning loss is 830772645.1341201\n",
            "364 learning loss is 830612397.2007653\n",
            "365 learning loss is 830455352.7875552\n",
            "366 learning loss is 830301447.7476245\n",
            "367 learning loss is 830150619.2208583\n",
            "368 learning loss is 830002805.6080292\n",
            "369 learning loss is 829857946.5454582\n",
            "370 learning loss is 829715982.8801814\n",
            "371 learning loss is 829576856.6456221\n",
            "372 learning loss is 829440511.0377481\n",
            "373 learning loss is 829306890.3917125\n",
            "374 learning loss is 829175940.1589637\n",
            "375 learning loss is 829047606.8848155\n",
            "376 learning loss is 828921838.1864703\n",
            "377 learning loss is 828798582.7314844\n",
            "378 learning loss is 828677790.2166657\n",
            "379 learning loss is 828559411.3473977\n",
            "380 learning loss is 828443397.8173795\n",
            "381 learning loss is 828329702.2887726\n",
            "382 learning loss is 828218278.3727486\n",
            "383 learning loss is 828109080.610428\n",
            "384 learning loss is 828002064.454202\n",
            "385 learning loss is 827897186.2494313\n",
            "386 learning loss is 827794403.2165126\n",
            "387 learning loss is 827693673.4333057\n",
            "388 learning loss is 827594955.8179163\n",
            "389 learning loss is 827498210.1118206\n",
            "390 learning loss is 827403396.8633357\n",
            "391 learning loss is 827310477.4114174\n",
            "392 learning loss is 827219413.8697864\n",
            "393 learning loss is 827130169.1113739\n",
            "394 learning loss is 827042706.7530792\n",
            "395 learning loss is 826956991.140834\n",
            "396 learning loss is 826872987.3349687\n",
            "397 learning loss is 826790661.0958707\n",
            "398 learning loss is 826709978.8699328\n",
            "399 learning loss is 826630907.7757843\n",
            "400 learning loss is 826553415.5907972\n",
            "401 learning loss is 826477470.7378669\n",
            "402 learning loss is 826403042.2724552\n",
            "403 learning loss is 826330099.8698964\n",
            "404 learning loss is 826258613.8129582\n",
            "405 learning loss is 826188554.9796513\n",
            "406 learning loss is 826119894.8312864\n",
            "407 learning loss is 826052605.4007692\n",
            "408 learning loss is 825986659.281132\n",
            "409 learning loss is 825922029.6142952\n",
            "410 learning loss is 825858690.0800557\n",
            "411 learning loss is 825796614.8852954\n",
            "412 learning loss is 825735778.7534072\n",
            "413 learning loss is 825676156.9139338\n",
            "414 learning loss is 825617725.0924139\n",
            "415 learning loss is 825560459.5004344\n",
            "416 learning loss is 825504336.8258799\n",
            "417 learning loss is 825449334.2233796\n",
            "418 learning loss is 825395429.304947\n",
            "419 learning loss is 825342600.1308055\n",
            "420 learning loss is 825290825.2004005\n",
            "421 learning loss is 825240083.4435908\n",
            "422 learning loss is 825190354.2120174\n",
            "423 learning loss is 825141617.2706453\n",
            "424 learning loss is 825093852.7894768\n",
            "425 learning loss is 825047041.3354285\n",
            "426 learning loss is 825001163.8643745\n",
            "427 learning loss is 824956201.7133466\n",
            "428 learning loss is 824912136.5928943\n",
            "429 learning loss is 824868950.579595\n",
            "430 learning loss is 824826626.1087168\n",
            "431 learning loss is 824785145.967028\n",
            "432 learning loss is 824744493.2857516\n",
            "433 learning loss is 824704651.5336595\n",
            "434 learning loss is 824665604.5103079\n",
            "435 learning loss is 824627336.339406\n",
            "436 learning loss is 824589831.462321\n",
            "437 learning loss is 824553074.6317098\n",
            "438 learning loss is 824517050.9052812\n",
            "439 learning loss is 824481745.6396825\n",
            "440 learning loss is 824447144.4845088\n",
            "441 learning loss is 824413233.3764333\n",
            "442 learning loss is 824379998.5334536\n",
            "443 learning loss is 824347426.4492557\n",
            "444 learning loss is 824315503.8876896\n",
            "445 learning loss is 824284217.8773571\n",
            "446 learning loss is 824253555.7063062\n",
            "447 learning loss is 824223504.916834\n",
            "448 learning loss is 824194053.3003939\n",
            "449 learning loss is 824165188.8926029\n",
            "450 learning loss is 824136899.9683509\n",
            "451 learning loss is 824109175.0370082\n",
            "452 learning loss is 824082002.8377281\n",
            "453 learning loss is 824055372.3348452\n",
            "454 learning loss is 824029272.7133642\n",
            "455 learning loss is 824003693.3745401\n",
            "456 learning loss is 823978623.9315486\n",
            "457 learning loss is 823954054.2052404\n",
            "458 learning loss is 823929974.2199829\n",
            "459 learning loss is 823906374.199585\n",
            "460 learning loss is 823883244.5633022\n",
            "461 learning loss is 823860575.9219239\n",
            "462 learning loss is 823838359.0739384\n",
            "463 learning loss is 823816585.001773\n",
            "464 learning loss is 823795244.8681135\n",
            "465 learning loss is 823774330.0122933\n",
            "466 learning loss is 823753831.9467576\n",
            "467 learning loss is 823733742.3535975\n",
            "468 learning loss is 823714053.0811543\n",
            "469 learning loss is 823694756.1406898\n",
            "470 learning loss is 823675843.703128\n",
            "471 learning loss is 823657308.0958556\n",
            "472 learning loss is 823639141.7995926\n",
            "473 learning loss is 823621337.4453224\n",
            "474 learning loss is 823603887.8112842\n",
            "475 learning loss is 823586785.8200259\n",
            "476 learning loss is 823570024.5355158\n",
            "477 learning loss is 823553597.1603128\n",
            "478 learning loss is 823537497.032792\n",
            "479 learning loss is 823521717.6244276\n",
            "480 learning loss is 823506252.5371294\n",
            "481 learning loss is 823491095.500632\n",
            "482 learning loss is 823476240.3699386\n",
            "483 learning loss is 823461681.1228122\n",
            "484 learning loss is 823447411.8573215\n",
            "485 learning loss is 823433426.789433\n",
            "486 learning loss is 823419720.2506523\n",
            "487 learning loss is 823406286.6857125\n",
            "488 learning loss is 823393120.6503087\n",
            "489 learning loss is 823380216.80888\n",
            "490 learning loss is 823367569.9324312\n",
            "491 learning loss is 823355174.8964039\n",
            "492 learning loss is 823343026.6785854\n",
            "493 learning loss is 823331120.357063\n",
            "494 learning loss is 823319451.1082175\n",
            "495 learning loss is 823308014.2047564\n",
            "496 learning loss is 823296805.0137882\n",
            "497 learning loss is 823285818.9949346\n",
            "498 learning loss is 823275051.6984792\n",
            "499 learning loss is 823264498.7635558\n",
            "500 learning loss is 823254155.9163705\n",
            "501 learning loss is 823244018.9684615\n",
            "502 learning loss is 823234083.8149921\n",
            "503 learning loss is 823224346.4330784\n",
            "504 learning loss is 823214802.8801514\n",
            "505 learning loss is 823205449.29235\n",
            "506 learning loss is 823196281.8829496\n",
            "507 learning loss is 823187296.940817\n",
            "508 learning loss is 823178490.8289009\n",
            "509 learning loss is 823169859.9827508\n",
            "510 learning loss is 823161400.9090654\n",
            "511 learning loss is 823153110.1842697\n",
            "512 learning loss is 823144984.4531224\n",
            "513 learning loss is 823137020.427349\n",
            "514 learning loss is 823129214.8843035\n",
            "515 learning loss is 823121564.6656568\n",
            "516 learning loss is 823114066.6761109\n",
            "517 learning loss is 823106717.8821393\n",
            "518 learning loss is 823099515.3107529\n",
            "519 learning loss is 823092456.0482887\n",
            "520 learning loss is 823085537.239226\n",
            "521 learning loss is 823078756.0850232\n",
            "522 learning loss is 823072109.8429798\n",
            "523 learning loss is 823065595.8251204\n",
            "524 learning loss is 823059211.3971004\n",
            "525 learning loss is 823052953.977136\n",
            "526 learning loss is 823046821.0349524\n",
            "527 learning loss is 823040810.0907558\n",
            "528 learning loss is 823034918.7142239\n",
            "529 learning loss is 823029144.5235181\n",
            "530 learning loss is 823023485.1843145\n",
            "531 learning loss is 823017938.4088553\n",
            "532 learning loss is 823012501.9550174\n",
            "533 learning loss is 823007173.6254022\n",
            "534 learning loss is 823001951.2664407\n",
            "535 learning loss is 822996832.7675192\n",
            "536 learning loss is 822991816.0601208\n",
            "537 learning loss is 822986899.1169848\n",
            "538 learning loss is 822982079.9512818\n",
            "539 learning loss is 822977356.6158069\n",
            "540 learning loss is 822972727.2021892\n",
            "541 learning loss is 822968189.840114\n",
            "542 learning loss is 822963742.6965647\n",
            "543 learning loss is 822959383.975077\n",
            "544 learning loss is 822955111.9150095\n",
            "545 learning loss is 822950924.7908279\n",
            "546 learning loss is 822946820.9114039\n",
            "547 learning loss is 822942798.6193291\n",
            "548 learning loss is 822938856.29024\n",
            "549 learning loss is 822934992.3321604\n",
            "550 learning loss is 822931205.1848526\n",
            "551 learning loss is 822927493.3191848\n",
            "552 learning loss is 822923855.2365105\n",
            "553 learning loss is 822920289.4680592\n",
            "554 learning loss is 822916794.5743395\n",
            "555 learning loss is 822913369.1445558\n",
            "556 learning loss is 822910011.7960346\n",
            "557 learning loss is 822906721.1736637\n",
            "558 learning loss is 822903495.9493412\n",
            "559 learning loss is 822900334.8214374\n",
            "560 learning loss is 822897236.5142653\n",
            "561 learning loss is 822894199.7775639\n",
            "562 learning loss is 822891223.38599\n",
            "563 learning loss is 822888306.1386213\n",
            "564 learning loss is 822885446.8584682\n",
            "565 learning loss is 822882644.3919972\n",
            "566 learning loss is 822879897.6086626\n",
            "567 learning loss is 822877205.4004474\n",
            "568 learning loss is 822874566.6814128\n",
            "569 learning loss is 822871980.3872603\n",
            "570 learning loss is 822869445.4748974\n",
            "571 learning loss is 822866960.9220148\n",
            "572 learning loss is 822864525.7266737\n",
            "573 learning loss is 822862138.9068965\n",
            "574 learning loss is 822859799.5002706\n",
            "575 learning loss is 822857506.5635574\n",
            "576 learning loss is 822855259.1723092\n",
            "577 learning loss is 822853056.4204954\n",
            "578 learning loss is 822850897.4201344\n",
            "579 learning loss is 822848781.3009337\n",
            "580 learning loss is 822846707.2099369\n",
            "581 learning loss is 822844674.311178\n",
            "582 learning loss is 822842681.7853426\n",
            "583 learning loss is 822840728.8294357\n",
            "584 learning loss is 822838814.6564556\n",
            "585 learning loss is 822836938.4950764\n",
            "586 learning loss is 822835099.589333\n",
            "587 learning loss is 822833297.1983181\n",
            "588 learning loss is 822831530.5958776\n",
            "589 learning loss is 822829799.0703205\n",
            "590 learning loss is 822828101.9241278\n",
            "591 learning loss is 822826438.4736707\n",
            "592 learning loss is 822824808.0489341\n",
            "593 learning loss is 822823209.9932443\n",
            "594 learning loss is 822821643.6630033\n",
            "595 learning loss is 822820108.4274287\n",
            "596 learning loss is 822818603.6682963\n",
            "597 learning loss is 822817128.7796925\n",
            "598 learning loss is 822815683.1677665\n",
            "599 learning loss is 822814266.2504901\n",
            "600 learning loss is 822812877.4574239\n",
            "601 learning loss is 822811516.2294838\n",
            "602 learning loss is 822810182.0187162\n",
            "603 learning loss is 822808874.2880752\n",
            "604 learning loss is 822807592.5112064\n",
            "605 learning loss is 822806336.1722316\n",
            "606 learning loss is 822805104.7655425\n",
            "607 learning loss is 822803897.7955945\n",
            "608 learning loss is 822802714.776706\n",
            "609 learning loss is 822801555.2328625\n",
            "610 learning loss is 822800418.6975242\n",
            "611 learning loss is 822799304.7134354\n",
            "612 learning loss is 822798212.8324419\n",
            "613 learning loss is 822797142.6153076\n",
            "614 learning loss is 822796093.6315379\n",
            "615 learning loss is 822795065.4592052\n",
            "616 learning loss is 822794057.6847775\n",
            "617 learning loss is 822793069.9029528\n",
            "618 learning loss is 822792101.7164929\n",
            "619 learning loss is 822791152.7360649\n",
            "620 learning loss is 822790222.5800818\n",
            "621 learning loss is 822789310.8745494\n",
            "622 learning loss is 822788417.2529143\n",
            "623 learning loss is 822787541.3559167\n",
            "624 learning loss is 822786682.8314437\n",
            "625 learning loss is 822785841.3343871\n",
            "626 learning loss is 822785016.5265054\n",
            "627 learning loss is 822784208.0762852\n",
            "628 learning loss is 822783415.6588084\n",
            "629 learning loss is 822782638.9556196\n",
            "630 learning loss is 822781877.6545986\n",
            "631 learning loss is 822781131.4498334\n",
            "632 learning loss is 822780400.0414969\n",
            "633 learning loss is 822779683.1357256\n",
            "634 learning loss is 822778980.4445012\n",
            "635 learning loss is 822778291.6855327\n",
            "636 learning loss is 822777616.5821446\n",
            "637 learning loss is 822776954.8631634\n",
            "638 learning loss is 822776306.2628083\n",
            "639 learning loss is 822775670.5205847\n",
            "640 learning loss is 822775047.3811773\n",
            "641 learning loss is 822774436.5943487\n",
            "642 learning loss is 822773837.914837\n",
            "643 learning loss is 822773251.1022575\n",
            "644 learning loss is 822772675.9210051\n",
            "645 learning loss is 822772112.1401598\n",
            "646 learning loss is 822771559.5333921\n",
            "647 learning loss is 822771017.878873\n",
            "648 learning loss is 822770486.9591837\n",
            "649 learning loss is 822769966.5612278\n",
            "650 learning loss is 822769456.476146\n",
            "651 learning loss is 822768956.4992301\n",
            "652 learning loss is 822768466.429842\n",
            "653 learning loss is 822767986.0713322\n",
            "654 learning loss is 822767515.2309597\n",
            "655 learning loss is 822767053.7198154\n",
            "656 learning loss is 822766601.3527445\n",
            "657 learning loss is 822766157.9482728\n",
            "658 learning loss is 822765723.328533\n",
            "659 learning loss is 822765297.3191931\n",
            "660 learning loss is 822764879.7493862\n",
            "661 learning loss is 822764470.4516404\n",
            "662 learning loss is 822764069.2618124\n",
            "663 learning loss is 822763676.0190219\n",
            "664 learning loss is 822763290.5655845\n",
            "665 learning loss is 822762912.7469499\n",
            "666 learning loss is 822762542.4116386\n",
            "667 learning loss is 822762179.4111817\n",
            "668 learning loss is 822761823.6000592\n",
            "669 learning loss is 822761474.8356444\n",
            "670 learning loss is 822761132.9781427\n",
            "671 learning loss is 822760797.8905381\n",
            "672 learning loss is 822760469.4385366\n",
            "673 learning loss is 822760147.4905118\n",
            "674 learning loss is 822759831.917453\n",
            "675 learning loss is 822759522.5929122\n",
            "676 learning loss is 822759219.392953\n",
            "677 learning loss is 822758922.1961014\n",
            "678 learning loss is 822758630.8832965\n",
            "679 learning loss is 822758345.3378423\n",
            "680 learning loss is 822758065.4453607\n",
            "681 learning loss is 822757791.0937451\n",
            "682 learning loss is 822757522.1731168\n",
            "683 learning loss is 822757258.5757782\n",
            "684 learning loss is 822757000.1961713\n",
            "685 learning loss is 822756746.9308344\n",
            "686 learning loss is 822756498.6783606\n",
            "687 learning loss is 822756255.3393568\n",
            "688 learning loss is 822756016.8164036\n",
            "689 learning loss is 822755783.0140164\n",
            "690 learning loss is 822755553.8386067\n",
            "691 learning loss is 822755329.1984445\n",
            "692 learning loss is 822755109.0036206\n",
            "693 learning loss is 822754893.1660116\n",
            "694 learning loss is 822754681.5992439\n",
            "695 learning loss is 822754474.2186583\n",
            "696 learning loss is 822754270.941277\n",
            "697 learning loss is 822754071.685769\n",
            "698 learning loss is 822753876.3724185\n",
            "699 learning loss is 822753684.923092\n",
            "700 learning loss is 822753497.2612071\n",
            "701 learning loss is 822753313.3117017\n",
            "702 learning loss is 822753133.0010033\n",
            "703 learning loss is 822752956.2570008\n",
            "704 learning loss is 822752783.0090134\n",
            "705 learning loss is 822752613.1877639\n",
            "706 learning loss is 822752446.7253498\n",
            "707 learning loss is 822752283.5552164\n",
            "708 learning loss is 822752123.6121302\n",
            "709 learning loss is 822751966.8321517\n",
            "710 learning loss is 822751813.1526107\n",
            "711 learning loss is 822751662.5120809\n",
            "712 learning loss is 822751514.8503548\n",
            "713 learning loss is 822751370.1084199\n",
            "714 learning loss is 822751228.2284337\n",
            "715 learning loss is 822751089.1537029\n",
            "716 learning loss is 822750952.8286581\n",
            "717 learning loss is 822750819.1988331\n",
            "718 learning loss is 822750688.2108417\n",
            "719 learning loss is 822750559.8123577\n",
            "720 learning loss is 822750433.9520923\n",
            "721 learning loss is 822750310.5797747\n",
            "722 learning loss is 822750189.6461318\n",
            "723 learning loss is 822750071.1028663\n",
            "724 learning loss is 822749954.902641\n",
            "725 learning loss is 822749840.9990569\n",
            "726 learning loss is 822749729.346635\n",
            "727 learning loss is 822749619.9007987\n",
            "728 learning loss is 822749512.617856\n",
            "729 learning loss is 822749407.4549809\n",
            "730 learning loss is 822749304.3701968\n",
            "731 learning loss is 822749203.3223603\n",
            "732 learning loss is 822749104.2711432\n",
            "733 learning loss is 822749007.1770175\n",
            "734 learning loss is 822748912.0012391\n",
            "735 learning loss is 822748818.7058321\n",
            "736 learning loss is 822748727.2535737\n",
            "737 learning loss is 822748637.6079792\n",
            "738 learning loss is 822748549.7332871\n",
            "739 learning loss is 822748463.5944456\n",
            "740 learning loss is 822748379.1570975\n",
            "741 learning loss is 822748296.3875669\n",
            "742 learning loss is 822748215.2528449\n",
            "743 learning loss is 822748135.7205782\n",
            "744 learning loss is 822748057.7590543\n",
            "745 learning loss is 822747981.3371893\n",
            "746 learning loss is 822747906.4245154\n",
            "747 learning loss is 822747832.9911695\n",
            "748 learning loss is 822747761.0078797\n",
            "749 learning loss is 822747690.4459543\n",
            "750 learning loss is 822747621.277271\n",
            "751 learning loss is 822747553.4742643\n",
            "752 learning loss is 822747487.0099155\n",
            "753 learning loss is 822747421.8577408\n",
            "754 learning loss is 822747357.9917818\n",
            "755 learning loss is 822747295.386594\n",
            "756 learning loss is 822747234.0172384\n",
            "757 learning loss is 822747173.8592683\n",
            "758 learning loss is 822747114.888723\n",
            "759 learning loss is 822747057.0821162\n",
            "760 learning loss is 822747000.4164267\n",
            "761 learning loss is 822746944.8690892\n",
            "762 learning loss is 822746890.4179866\n",
            "763 learning loss is 822746837.0414387\n",
            "764 learning loss is 822746784.7181958\n",
            "765 learning loss is 822746733.4274287\n",
            "766 learning loss is 822746683.1487212\n",
            "767 learning loss is 822746633.8620613\n",
            "768 learning loss is 822746585.5478328\n",
            "769 learning loss is 822746538.1868099\n",
            "770 learning loss is 822746491.7601454\n",
            "771 learning loss is 822746446.2493672\n",
            "772 learning loss is 822746401.6363683\n",
            "773 learning loss is 822746357.9034003\n",
            "774 learning loss is 822746315.0330664\n",
            "775 learning loss is 822746273.0083145\n",
            "776 learning loss is 822746231.8124304\n",
            "777 learning loss is 822746191.42903\n",
            "778 learning loss is 822746151.8420547\n",
            "779 learning loss is 822746113.0357636\n",
            "780 learning loss is 822746074.9947269\n",
            "781 learning loss is 822746037.7038213\n",
            "782 learning loss is 822746001.1482221\n",
            "783 learning loss is 822745965.3133988\n",
            "784 learning loss is 822745930.185108\n",
            "785 learning loss is 822745895.7493887\n",
            "786 learning loss is 822745861.9925563\n",
            "787 learning loss is 822745828.901197\n",
            "788 learning loss is 822745796.4621623\n",
            "789 learning loss is 822745764.6625642\n",
            "790 learning loss is 822745733.4897702\n",
            "791 learning loss is 822745702.9313972\n",
            "792 learning loss is 822745672.9753077\n",
            "793 learning loss is 822745643.6096041\n",
            "794 learning loss is 822745614.8226247\n",
            "795 learning loss is 822745586.6029382\n",
            "796 learning loss is 822745558.9393394\n",
            "797 learning loss is 822745531.8208457\n",
            "798 learning loss is 822745505.236691\n",
            "799 learning loss is 822745479.1763223\n",
            "800 learning loss is 822745453.6293962\n",
            "801 learning loss is 822745428.585773\n",
            "802 learning loss is 822745404.0355141\n",
            "803 learning loss is 822745379.9688778\n",
            "804 learning loss is 822745356.3763144\n",
            "805 learning loss is 822745333.2484639\n",
            "806 learning loss is 822745310.5761513\n",
            "807 learning loss is 822745288.3503828\n",
            "808 learning loss is 822745266.5623426\n",
            "809 learning loss is 822745245.2033898\n",
            "810 learning loss is 822745224.2650534\n",
            "811 learning loss is 822745203.7390314\n",
            "812 learning loss is 822745183.6171846\n",
            "813 learning loss is 822745163.8915356\n",
            "814 learning loss is 822745144.5542651\n",
            "815 learning loss is 822745125.5977069\n",
            "816 learning loss is 822745107.0143481\n",
            "817 learning loss is 822745088.7968236\n",
            "818 learning loss is 822745070.9379143\n",
            "819 learning loss is 822745053.4305433\n",
            "820 learning loss is 822745036.2677737\n",
            "821 learning loss is 822745019.4428071\n",
            "822 learning loss is 822745002.9489775\n",
            "823 learning loss is 822744986.7797517\n",
            "824 learning loss is 822744970.9287263\n",
            "825 learning loss is 822744955.3896232\n",
            "826 learning loss is 822744940.1562895\n",
            "827 learning loss is 822744925.2226928\n",
            "828 learning loss is 822744910.5829215\n",
            "829 learning loss is 822744896.2311795\n",
            "830 learning loss is 822744882.161786\n",
            "831 learning loss is 822744868.3691719\n",
            "832 learning loss is 822744854.8478787\n",
            "833 learning loss is 822744841.5925559\n",
            "834 learning loss is 822744828.5979581\n",
            "835 learning loss is 822744815.8589439\n",
            "836 learning loss is 822744803.370474\n",
            "837 learning loss is 822744791.1276078\n",
            "838 learning loss is 822744779.125503\n",
            "839 learning loss is 822744767.3594124\n",
            "840 learning loss is 822744755.8246833\n",
            "841 learning loss is 822744744.5167543\n",
            "842 learning loss is 822744733.4311547\n",
            "843 learning loss is 822744722.5635015\n",
            "844 learning loss is 822744711.9094987\n",
            "845 learning loss is 822744701.4649351\n",
            "846 learning loss is 822744691.225683\n",
            "847 learning loss is 822744681.1876953\n",
            "848 learning loss is 822744671.3470061\n",
            "849 learning loss is 822744661.6997268\n",
            "850 learning loss is 822744652.2420459\n",
            "851 learning loss is 822744642.9702276\n",
            "852 learning loss is 822744633.8806095\n",
            "853 learning loss is 822744624.9696015\n",
            "854 learning loss is 822744616.2336844\n",
            "855 learning loss is 822744607.6694088\n",
            "856 learning loss is 822744599.2733934\n",
            "857 learning loss is 822744591.042323\n",
            "858 learning loss is 822744582.9729486\n",
            "859 learning loss is 822744575.0620852\n",
            "860 learning loss is 822744567.3066102\n",
            "861 learning loss is 822744559.7034634\n",
            "862 learning loss is 822744552.2496445\n",
            "863 learning loss is 822744544.9422122\n",
            "864 learning loss is 822744537.7782843\n",
            "865 learning loss is 822744530.7550347\n",
            "866 learning loss is 822744523.8696926\n",
            "867 learning loss is 822744517.1195433\n",
            "868 learning loss is 822744510.5019244\n",
            "869 learning loss is 822744504.0142267\n",
            "870 learning loss is 822744497.6538922\n",
            "871 learning loss is 822744491.4184135\n",
            "872 learning loss is 822744485.3053328\n",
            "873 learning loss is 822744479.3122408\n",
            "874 learning loss is 822744473.4367754\n",
            "875 learning loss is 822744467.6766213\n",
            "876 learning loss is 822744462.0295095\n",
            "877 learning loss is 822744456.4932144\n",
            "878 learning loss is 822744451.0655558\n",
            "879 learning loss is 822744445.7443951\n",
            "880 learning loss is 822744440.5276372\n",
            "881 learning loss is 822744435.4132274\n",
            "882 learning loss is 822744430.3991522\n",
            "883 learning loss is 822744425.4834372\n",
            "884 learning loss is 822744420.6641471\n",
            "885 learning loss is 822744415.9393852\n",
            "886 learning loss is 822744411.307292\n",
            "887 learning loss is 822744406.766044\n",
            "888 learning loss is 822744402.3138548\n",
            "889 learning loss is 822744397.9489726\n",
            "890 learning loss is 822744393.6696799\n",
            "891 learning loss is 822744389.4742937\n",
            "892 learning loss is 822744385.3611636\n",
            "893 learning loss is 822744381.3286719\n",
            "894 learning loss is 822744377.3752334\n",
            "895 learning loss is 822744373.4992933\n",
            "896 learning loss is 822744369.699328\n",
            "897 learning loss is 822744365.9738435\n",
            "898 learning loss is 822744362.3213755\n",
            "899 learning loss is 822744358.7404885\n",
            "900 learning loss is 822744355.2297758\n",
            "901 learning loss is 822744351.7878573\n",
            "902 learning loss is 822744348.4133812\n",
            "903 learning loss is 822744345.105022\n",
            "904 learning loss is 822744341.8614796\n",
            "905 learning loss is 822744338.6814809\n",
            "906 learning loss is 822744335.5637769\n",
            "907 learning loss is 822744332.5071433\n",
            "908 learning loss is 822744329.51038\n",
            "909 learning loss is 822744326.5723108\n",
            "910 learning loss is 822744323.6917821\n",
            "911 learning loss is 822744320.8676645\n",
            "912 learning loss is 822744318.0988486\n",
            "913 learning loss is 822744315.3842483\n",
            "914 learning loss is 822744312.7227987\n",
            "915 learning loss is 822744310.113456\n",
            "916 learning loss is 822744307.5551965\n",
            "917 learning loss is 822744305.0470171\n",
            "918 learning loss is 822744302.5879341\n",
            "919 learning loss is 822744300.1769834\n",
            "920 learning loss is 822744297.8132197\n",
            "921 learning loss is 822744295.4957165\n",
            "922 learning loss is 822744293.2235656\n",
            "923 learning loss is 822744290.9958763\n",
            "924 learning loss is 822744288.8117763\n",
            "925 learning loss is 822744286.6704088\n",
            "926 learning loss is 822744284.5709358\n",
            "927 learning loss is 822744282.512535\n",
            "928 learning loss is 822744280.4943998\n",
            "929 learning loss is 822744278.5157397\n",
            "930 learning loss is 822744276.5757806\n",
            "931 learning loss is 822744274.6737629\n",
            "932 learning loss is 822744272.8089414\n",
            "933 learning loss is 822744270.9805866\n",
            "934 learning loss is 822744269.1879833\n",
            "935 learning loss is 822744267.4304296\n",
            "936 learning loss is 822744265.707238\n",
            "937 learning loss is 822744264.0177343\n",
            "938 learning loss is 822744262.3612576\n",
            "939 learning loss is 822744260.7371606\n",
            "940 learning loss is 822744259.1448072\n",
            "941 learning loss is 822744257.5835755\n",
            "942 learning loss is 822744256.0528551\n",
            "943 learning loss is 822744254.5520475\n",
            "944 learning loss is 822744253.0805659\n",
            "945 learning loss is 822744251.6378356\n",
            "946 learning loss is 822744250.2232927\n",
            "947 learning loss is 822744248.8363843\n",
            "948 learning loss is 822744247.4765692\n",
            "949 learning loss is 822744246.1433157\n",
            "950 learning loss is 822744244.8361034\n",
            "951 learning loss is 822744243.5544217\n",
            "952 learning loss is 822744242.2977699\n",
            "953 learning loss is 822744241.0656585\n",
            "954 learning loss is 822744239.8576051\n",
            "955 learning loss is 822744238.6731396\n",
            "956 learning loss is 822744237.5117987\n",
            "957 learning loss is 822744236.3731298\n",
            "958 learning loss is 822744235.2566888\n",
            "959 learning loss is 822744234.1620396\n",
            "960 learning loss is 822744233.0887558\n",
            "961 learning loss is 822744232.0364188\n",
            "962 learning loss is 822744231.0046183\n",
            "963 learning loss is 822744229.9929518\n",
            "964 learning loss is 822744229.0010251\n",
            "965 learning loss is 822744228.0284516\n",
            "966 learning loss is 822744227.0748522\n",
            "967 learning loss is 822744226.1398556\n",
            "968 learning loss is 822744225.2230972\n",
            "969 learning loss is 822744224.3242203\n",
            "970 learning loss is 822744223.4428744\n",
            "971 learning loss is 822744222.5787164\n",
            "972 learning loss is 822744221.7314099\n",
            "973 learning loss is 822744220.9006251\n",
            "974 learning loss is 822744220.0860385\n",
            "975 learning loss is 822744219.2873331\n",
            "976 learning loss is 822744218.5041978\n",
            "977 learning loss is 822744217.7363285\n",
            "978 learning loss is 822744216.983426\n",
            "979 learning loss is 822744216.2451977\n",
            "980 learning loss is 822744215.5213563\n",
            "981 learning loss is 822744214.8116202\n",
            "982 learning loss is 822744214.1157138\n",
            "983 learning loss is 822744213.4333663\n",
            "984 learning loss is 822744212.7643126\n",
            "985 learning loss is 822744212.1082927\n",
            "986 learning loss is 822744211.4650518\n",
            "987 learning loss is 822744210.8343395\n",
            "988 learning loss is 822744210.2159113\n",
            "989 learning loss is 822744209.6095269\n",
            "990 learning loss is 822744209.0149504\n",
            "991 learning loss is 822744208.4319516\n",
            "992 learning loss is 822744207.8603035\n",
            "993 learning loss is 822744207.2997848\n",
            "994 learning loss is 822744206.7501776\n",
            "995 learning loss is 822744206.2112685\n",
            "996 learning loss is 822744205.6828488\n",
            "997 learning loss is 822744205.1647133\n",
            "998 learning loss is 822744204.6566609\n",
            "999 learning loss is 822744204.1584947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = slr.predict(X_test)\n",
        "conflict = mean_squared_error(y_test, pred)\n",
        "print(\"MSE of ScratchLinearRegression:{}\".format(conflict))\n",
        "\n",
        "\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "pred = reg.predict(X_test)\n",
        "conflict = mean_squared_error(y_test, pred)\n",
        "print(\"MSE of sklearn Linear Regression:{}\".format(conflict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCLXQ-bTnMlE",
        "outputId": "870acb4e-7cb3-4e8e-ceda-5c6f7ff51672"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE of ScratchLinearRegression:1796867628.5140812\n",
            "MSE of sklearn Linear Regression:1796799357.5447416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------Problem 7-----------#\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss, label = 'Training loss')\n",
        "plt.plot(slr.val_loss, label = 'Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RNYuqkmunVxd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3dd8eba0-abba-4359-8191-a0d5c121ec5a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}